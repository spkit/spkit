{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Decision Trees without converting Catogorical Features\n\n\nDecision Trees without converting Catogorical Features using SpKit\n\nMost of ML libraries force us to convert the catogorycal features into one-hot vector or any numerical \nvalue. However, it should not be the case, **Not atleast with Decision Trees**, due a simple reason, \nof how decision tree works. In **spkit library**, Decision tree can handle mixed type input features, \n'Catogorical' and 'Numerical'. In this notebook, I would use a dataset *hurricNamed* from *vincentarelbundock* \ngithub repository, and use only a few features, mixed of catogorical and numerical features. \nConverting number of deaths to binary with threshold of 10, we handle this as Classification Problem. \nHowever, it is not shown that coverting features into one-hot vector or any label encoder affects the \nperformance of model, but, it is useful, when you need to visulize the decision process. \nVery important when you need to extract and simplify the decision rule.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport spkit\nprint('spkit version :', spkit.__version__)\n\n\n# just to ensure the reproducible results\nnp.random.seed(100) \n\n# ## Classification  - binary class - hurricNamed Dataset\n\nfrom spkit.ml import ClassificationTree\n\n\nD = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/DAAG/hurricNamed.csv')\nfeature_names = ['Name', 'AffectedStates','LF.WindsMPH','mf']#,'BaseDamage' 'Year','Year','LF.WindsMPH', 'LF.PressureMB','LF.times',\n\n\nX = np.array(D[feature_names])\nX[:,1] = [st.split(',')[0] for st in X[:,1]] #Choosing only first name of state from AffectedStates feature\ny = np.array(D[['deaths']])[:,0]\n\n# Converting target into binary with threshold of 10 deaths\ny[y<10] =0\ny[y>=10]=1\n\n\nprint('Shapes',X.shape,y.shape)\n\n\n# ## Training, \n\n# Not doing training and testing, as objective is to show that it works\n\nmodel = ClassificationTree(max_depth=4)\nmodel.fit(X,y,feature_names=feature_names)\nyp = model.predict(X)\n\n\nplt.figure(figsize=(8,5))\nmodel.plotTree()\nplt.tight_layout()\nplt.show()\n\n\n# Here, it can be seen that first feature 'LF.WindsMPH' is a numerical, thus the threshold is greater then equal to, however for catogorical features like 'Name' and 'AffectedStates'threshold is equal to only."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}